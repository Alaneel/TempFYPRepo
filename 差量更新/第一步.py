import requests
import json
import time
import os
from loguru import logger
import re
from func_timeout import func_set_timeout
import urllib3

urllib3.disable_warnings()
import sqlite3
import pandas as pd
from datetime import datetime

logger.add("logs/propertyguru_1_incremental.log", level="INFO")


class propertyguru:

    def __init__(self):
        self.apikey = ''
        self.proxy = ''
        self.data_dir = "data"
        self.html_dir = os.path.join(self.data_dir, "html")
        self.json_dir = os.path.join(self.data_dir, "json")

        os.makedirs(self.html_dir, exist_ok=True)
        os.makedirs(self.json_dir, exist_ok=True)
        os.makedirs(self.data_dir, exist_ok=True)

        self.db_path = os.path.join(self.data_dir, "propertyguru_1.db")

        # è¿ç»­æ— æ–°è®°å½•é¡µæ•°é˜ˆå€¼ï¼ˆæ”¹è¿›åçš„æ—©åœæ¡ä»¶ï¼‰
        self.PAGES_WITHOUT_NEW_THRESHOLD = 5

        # æ—¶é—´çª—å£é˜ˆå€¼ï¼ˆå¤©æ•°ï¼‰
        self.TIME_WINDOW_DAYS = 3

        # å›æº¯æ£€æŸ¥é¡µæ•°
        self.REVIEW_PAGES = 10

        self.new_count = 0
        self.update_count = 0
        self.skip_count = 0
        self.fail_count = 0

        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“ï¼Œåˆ›å»ºè¡¨ç»“æ„"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ä¸»æ•°æ®è¡¨
            cursor.execute('''
                           CREATE TABLE IF NOT EXISTS propertyguru
                           (
                               ID
                               TEXT,
                               localizedTitle
                               TEXT,
                               fullAddress
                               TEXT,
                               price_pretty
                               TEXT,
                               beds
                               TEXT,
                               baths
                               TEXT,
                               area_sqft
                               TEXT,
                               price_psf
                               TEXT,
                               nearbyText
                               TEXT,
                               built_year
                               TEXT,
                               property_type
                               TEXT,
                               tenure
                               TEXT,
                               url_path
                               TEXT
                               PRIMARY
                               KEY,
                               recency_text
                               TEXT,
                               agent_id
                               TEXT,
                               agent_name
                               TEXT,
                               agent_description
                               TEXT,
                               agent_url_path
                               TEXT,
                               CEA
                               TEXT,
                               mobile
                               TEXT,
                               rating
                               TEXT,
                               buy_rent
                               TEXT,
                               created_at
                               TIMESTAMP
                               DEFAULT
                               CURRENT_TIMESTAMP,
                               updated_at
                               TIMESTAMP
                               DEFAULT
                               CURRENT_TIMESTAMP
                           )
                           ''')

            # çˆ¬è™«è®°å½•è¡¨
            cursor.execute('''
                           CREATE TABLE IF NOT EXISTS propertyguru_spider
                           (
                               url_path
                               TEXT
                               PRIMARY
                               KEY,
                               status
                               TEXT,
                               crawled_at
                               TIMESTAMP
                               DEFAULT
                               CURRENT_TIMESTAMP
                           )
                           ''')

            # çˆ¬å–è¿›åº¦è¡¨
            cursor.execute('''
                           CREATE TABLE IF NOT EXISTS crawl_progress
                           (
                               category
                               TEXT
                               PRIMARY
                               KEY,
                               last_page
                               INTEGER,
                               total_pages
                               INTEGER,
                               last_update
                               TIMESTAMP
                               DEFAULT
                               CURRENT_TIMESTAMP
                           )
                           ''')

            conn.commit()
            logger.success(f"æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ: {self.db_path}")

        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        finally:
            if conn:
                conn.close()

    def get_last_crawl_time(self, category):
        """è·å–ä¸Šæ¬¡çˆ¬å–æ—¶é—´"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute(
                "SELECT last_update FROM crawl_progress WHERE category = ?",
                (category,)
            )
            result = cursor.fetchone()

            if result and result[0]:
                return datetime.fromisoformat(result[0])
            else:
                # å¦‚æœæ²¡æœ‰è®°å½•ï¼Œè¿”å›ä¸€ä¸ªå¾ˆä¹…ä»¥å‰çš„æ—¶é—´
                return datetime(2000, 1, 1)

        except Exception as e:
            logger.error(f"è·å–ä¸Šæ¬¡çˆ¬å–æ—¶é—´å¤±è´¥: {str(e)}")
            return datetime(2000, 1, 1)
        finally:
            if conn:
                conn.close()

    def get_crawl_progress(self, category):
        """è·å–çˆ¬å–è¿›åº¦ï¼Œè€ƒè™‘æ—¶é—´çª—å£"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute(
                "SELECT last_page, last_update FROM crawl_progress WHERE category = ?",
                (category,)
            )
            result = cursor.fetchone()

            if result:
                last_page, last_update = result[0], result[1]
                last_update_time = datetime.fromisoformat(last_update)
                days_ago = (datetime.now() - last_update_time).days

                # å¦‚æœè¶…è¿‡æ—¶é—´çª—å£ï¼Œé‡æ–°ä»ç¬¬1é¡µå¼€å§‹
                if days_ago > self.TIME_WINDOW_DAYS:
                    logger.warning(
                        f"ä¸Šæ¬¡æ›´æ–°å·²è¿‡å» {days_ago} å¤©ï¼ˆé˜ˆå€¼: {self.TIME_WINDOW_DAYS}å¤©ï¼‰ï¼Œé‡æ–°å…¨é‡çˆ¬å–"
                    )
                    return 1, None

                logger.info(f"ç»§ç»­ä¸Šæ¬¡è¿›åº¦ï¼Œä»ç¬¬ {last_page} é¡µå¼€å§‹")
                return last_page, None

            logger.info(f"é¦–æ¬¡çˆ¬å– {category}")
            return 1, None

        except Exception as e:
            logger.error(f"è·å–çˆ¬å–è¿›åº¦å¤±è´¥: {str(e)}")
            return 1, None
        finally:
            if conn:
                conn.close()

    def update_crawl_progress(self, category, last_page, total_pages=None):
        """æ›´æ–°çˆ¬å–è¿›åº¦"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            if total_pages:
                cursor.execute(
                    "INSERT OR REPLACE INTO crawl_progress (category, last_page, total_pages, last_update) VALUES (?, ?, ?, ?)",
                    (category, last_page, total_pages, datetime.now())
                )
            else:
                cursor.execute(
                    "INSERT OR REPLACE INTO crawl_progress (category, last_page, last_update) VALUES (?, ?, ?)",
                    (category, last_page, datetime.now())
                )
            conn.commit()
            logger.debug(f"æ›´æ–°çˆ¬å–è¿›åº¦: {category} ç¬¬ {last_page} é¡µ")
        except Exception as e:
            logger.error(f"æ›´æ–°çˆ¬å–è¿›åº¦å¤±è´¥: {str(e)}")
        finally:
            if conn:
                conn.close()

    def insert_spider_record(self, url_path, status):
        """å‘çˆ¬è™«è®°å½•è¡¨ä¸­æ’å…¥è®°å½•"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute(
                "INSERT OR REPLACE INTO propertyguru_spider (url_path, status, crawled_at) VALUES (?, ?, ?)",
                (url_path, status, datetime.now())
            )
            conn.commit()
        except Exception as e:
            logger.error(f"çˆ¬è™«è®°å½•æ’å…¥å¤±è´¥: {url_path}, é”™è¯¯: {str(e)}")
        finally:
            if conn:
                conn.close()

    def check_spider_record(self, url_path, force_update=False):
        """æ£€æŸ¥çˆ¬è™«è®°å½•è¡¨ä¸­æ˜¯å¦å­˜åœ¨è®°å½•"""
        if force_update:
            return False  # å¼ºåˆ¶æ›´æ–°æ¨¡å¼ï¼Œè·³è¿‡æ£€æŸ¥

        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT status FROM propertyguru_spider WHERE url_path = ?", (url_path,))
            result = cursor.fetchone()
            return result is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥çˆ¬è™«è®°å½•å¤±è´¥: {url_path}, é”™è¯¯: {str(e)}")
            return False
        finally:
            if conn:
                conn.close()

    def insert_record(self, result, force_update=False):
        """å‘æ•°æ®åº“ä¸­æ’å…¥æˆ–æ›´æ–°è®°å½•"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            url_path = result.get("url_path", 'æ— url_path')
            cursor.execute("SELECT * FROM propertyguru WHERE url_path = ?", (url_path,))
            existing = cursor.fetchone()

            if existing:
                if force_update:
                    # å¼ºåˆ¶æ›´æ–°æ¨¡å¼ï¼šæ›´æ–°æ‰€æœ‰å­—æ®µ
                    cursor.execute('''
                                   UPDATE propertyguru
                                   SET ID=?,
                                       localizedTitle=?,
                                       fullAddress=?,
                                       price_pretty=?,
                                       beds=?,
                                       baths=?,
                                       area_sqft=?,
                                       price_psf=?,
                                       nearbyText=?,
                                       built_year=?,
                                       property_type=?,
                                       tenure=?,
                                       recency_text=?,
                                       agent_id=?,
                                       agent_name=?,
                                       agent_description=?,
                                       agent_url_path=?,
                                       CEA=?,
                                       mobile=?,
                                       rating=?,
                                       buy_rent=?,
                                       updated_at=?
                                   WHERE url_path = ?
                                   ''', (
                                       result.get("ID", 'æ— id'),
                                       result.get("localizedTitle", 'æ— æ ‡é¢˜'),
                                       result.get("fullAddress", 'æ— åœ°å€'),
                                       result.get("price_pretty", 'æ— ä»·æ ¼'),
                                       result.get("beds", 'æ— åºŠæ•°'),
                                       result.get("baths", 'æ— æµ´å®¤æ•°'),
                                       result.get("area_sqft", 'æ— é¢ç§¯'),
                                       result.get("price_psf", 'æ— æ¯å¹³æ–¹è‹±å°ºä»·æ ¼'),
                                       result.get("nearbyText", 'æ— åœ°é“'),
                                       result.get("built_year", 'æ— å»ºé€ å¹´ä»½'),
                                       result.get("property_type", 'æ— ç‰©ä¸šç±»å‹'),
                                       result.get("tenure", 'æ— äº§æƒ'),
                                       result.get("recency_text", 'æ— æ›´æ–°æ—¶é—´'),
                                       result.get("agent_id", 'æ— id'),
                                       result.get("agent_name", 'æ— åå­—'),
                                       result.get("agent_description", 'æ— æè¿°'),
                                       result.get("agent_url_path", 'æ— url_path'),
                                       result.get("CEA", ''),
                                       result.get("mobile", ''),
                                       result.get("rating", ''),
                                       result.get("buy_rent", 'æ— buy_rent'),
                                       datetime.now(),
                                       url_path
                                   ))
                    logger.info(f"è®°å½•å¼ºåˆ¶æ›´æ–°: {url_path}")
                    self.update_count += 1
                else:
                    # æ™®é€šæ¨¡å¼ï¼šè®°å½•å·²å­˜åœ¨ï¼Œè·³è¿‡
                    logger.debug(f"è®°å½•å·²å­˜åœ¨ï¼Œè·³è¿‡: {url_path}")
                    self.skip_count += 1
                    return False
            else:
                # æ’å…¥æ–°è®°å½•
                cursor.execute('''
                               INSERT INTO propertyguru (ID, localizedTitle, fullAddress, price_pretty, beds, baths,
                                                         area_sqft, price_psf, nearbyText, built_year, property_type,
                                                         tenure, url_path, recency_text, agent_id, agent_name,
                                                         agent_description, agent_url_path, CEA, mobile, rating,
                                                         buy_rent)
                               VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                               ''', (
                                   result.get("ID", 'æ— id'),
                                   result.get("localizedTitle", 'æ— æ ‡é¢˜'),
                                   result.get("fullAddress", 'æ— åœ°å€'),
                                   result.get("price_pretty", 'æ— ä»·æ ¼'),
                                   result.get("beds", 'æ— åºŠæ•°'),
                                   result.get("baths", 'æ— æµ´å®¤æ•°'),
                                   result.get("area_sqft", 'æ— é¢ç§¯'),
                                   result.get("price_psf", 'æ— æ¯å¹³æ–¹è‹±å°ºä»·æ ¼'),
                                   result.get("nearbyText", 'æ— åœ°é“'),
                                   result.get("built_year", 'æ— å»ºé€ å¹´ä»½'),
                                   result.get("property_type", 'æ— ç‰©ä¸šç±»å‹'),
                                   result.get("tenure", 'æ— äº§æƒ'),
                                   url_path,
                                   result.get("recency_text", 'æ— æ›´æ–°æ—¶é—´'),
                                   result.get("agent_id", 'æ— id'),
                                   result.get("agent_name", 'æ— åå­—'),
                                   result.get("agent_description", 'æ— æè¿°'),
                                   result.get("agent_url_path", 'æ— url_path'),
                                   result.get("CEA", ''),
                                   result.get("mobile", ''),
                                   result.get("rating", ''),
                                   result.get("buy_rent", 'æ— buy_rent')
                               ))
                logger.info(f"è®°å½•æ’å…¥æˆåŠŸ: {url_path}")
                self.new_count += 1

            conn.commit()
            return True

        except Exception as e:
            logger.error(f"è®°å½•æ“ä½œå¤±è´¥: {url_path}, é”™è¯¯: {str(e)}")
            return False
        finally:
            if conn:
                conn.close()

    def check_record_exists(self, url_path):
        """æ£€æŸ¥ url è®°å½•æ˜¯å¦å­˜åœ¨"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT url_path FROM propertyguru WHERE url_path = ?", (url_path,))
            result = cursor.fetchone()
            return result is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥è®°å½•å¤±è´¥: {url_path}, é”™è¯¯: {str(e)}")
            return False
        finally:
            if conn:
                conn.close()

    def add_failed_page(self, url_path, error_msg, retry_count=0):
        """è®°å½•å¤±è´¥çš„é¡µé¢ï¼Œåç»­é‡è¯•"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute('''
                INSERT OR REPLACE INTO failed_pages 
                (url_path, error_message, retry_count, last_attempt) 
                VALUES (?, ?, ?, ?)
            ''', (url_path, error_msg, retry_count, datetime.now()))
            conn.commit()
        except Exception as e:
            logger.error(f"è®°å½•å¤±è´¥é¡µé¢å¤±è´¥: {url_path}, é”™è¯¯: {str(e)}")
        finally:
            if conn:
                conn.close()

    @func_set_timeout(60)
    def get_request(self, method, url, headers):
        return requests.request(method, url, headers=headers, verify=False)

    def fetch(self, url_path, max_try=3):
        """è¯·æ±‚ç½‘é¡µ"""
        for attempt in range(max_try):
            try:
                url = f"https://api.cloudbypass.com/{url_path}"
                method = "GET"
                headers = {
                    "x-cb-apikey": f"{self.apikey}",
                    "x-cb-host": r"www.propertyguru.com.sg",
                    "x-cb-version": r"2",
                    "x-cb-part": r"0",
                    "x-cb-fp": r"chrome",
                    "x-cb-proxy": f"{self.proxy}",
                }

                response = self.get_request(method, url, headers)

                if response and response.status_code == 200:
                    return response
                else:
                    error_msg = f"è¯·æ±‚å¤±è´¥: status_code={response.status_code if response else 'N/A'}"
                    logger.error(f"{error_msg} ç¬¬ {attempt + 1} æ¬¡: {url_path}")
                    if response:
                        code = response.json().get('code')
                        if code in ['CLOUDFLARE_CHALLENGE_TIMEOUT']:
                            continue
                        if code in ["PROXY_CONNECT_ABORTED", 'APIKEY_INVALID', 'INSUFFICIENT_BALANCE']:
                            logger.error(f"è‡´å‘½é”™è¯¯: {url_path} - {response.text}")
                            self.add_failed_page(url_path, response.text, attempt + 1)
                            os._exit(0)
                    self.add_failed_page(url_path, error_msg, attempt + 1)
            except Exception as e:
                error_msg = f"è¯·æ±‚å¼‚å¸¸: {str(e)}"
                logger.error(f"{error_msg} ç¬¬ {attempt + 1} æ¬¡: {url_path}")
                self.add_failed_page(url_path, error_msg, attempt + 1)
                continue
        self.fail_count += 1
        return None

    def analysis_list_page(self, response, page, html_name, force_update=False):
        """è§£æåˆ—è¡¨é¡µ"""
        consecutive_exists = 0  # è¿ç»­å­˜åœ¨çš„è®°å½•æ•°
        new_records = 0  # æ–°è®°å½•æ•°

        with open(os.path.join(self.html_dir, f'{html_name}_page_{page}.html'), 'w', encoding='utf-8') as f:
            f.write(response.text)

        data_json = re.findall('<script id="__NEXT_DATA__" type="application/json".*?>(.*?)</script>', response.text,
                               re.S)
        if not data_json:
            logger.error(f"data_json è·å–å¤±è´¥ï¼š{page}")
            return consecutive_exists, new_records

        data_json = json.loads(data_json[0])

        with open(os.path.join(self.json_dir, f'{html_name}_page_{page}.json'), 'w', encoding='utf-8') as f:
            json.dump(data_json, f, ensure_ascii=False, indent=4)

        listingsData = data_json.get('props', {}).get('pageProps', {}).get('pageData', {}).get('data', {}).get(
            'listingsData', [])
        logger.info(f"{html_name} {page}é¡µæ•°æ®æ•°é‡ï¼š{len(listingsData)}")

        for item in listingsData:
            listingData = item.get('listingData', {})
            url_path = listingData.get("url", "").replace('https://www.propertyguru.com.sg/', '')

            # æ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨ï¼ˆå¼ºåˆ¶æ›´æ–°æ¨¡å¼ä¼šå¿½ç•¥æ£€æŸ¥ï¼‰
            if not force_update and self.check_record_exists(url_path):
                consecutive_exists += 1
                logger.debug(f"è®°å½•å·²å­˜åœ¨: {url_path} (è¿ç»­ç¬¬{consecutive_exists}æ¡)")
                continue
            else:
                consecutive_exists = 0  # é‡ç½®è®¡æ•°å™¨
                new_records += 1

            # æå–æ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ä¿æŒä¸å˜ï¼‰
            id_ = listingData.get('id', 'æ— id')
            localizedTitle = listingData.get('localizedTitle', 'æ— æ ‡é¢˜')
            fullAddress = listingData.get('fullAddress', 'æ— åœ°å€')
            price_pretty = listingData.get('price', {}).get('pretty', 'æ— ä»·æ ¼')

            beds = "æœªçŸ¥"
            baths = "æœªçŸ¥"
            area_sqft = "æœªçŸ¥"
            price_psf = "æœªçŸ¥"

            bedrooms = listingData.get('bedrooms')
            if bedrooms is not None and bedrooms >= 0:
                beds = f"{bedrooms} Beds"

            bathrooms = listingData.get('bathrooms')
            if bathrooms is not None and bathrooms >= 0:
                baths = f"{bathrooms} Baths"

            floorArea = listingData.get('floorArea')
            if floorArea:
                area_sqft = f"{floorArea} sqft"

            pricePerArea = listingData.get('pricePerArea', {}).get('localeStringValue')
            if pricePerArea:
                price_psf = f"S$ {pricePerArea} psf"

            listingFeatures = listingData.get('listingFeatures', [])
            if listingFeatures:
                for feature_item in listingFeatures:
                    if isinstance(feature_item, list):
                        for sub_feature in feature_item:
                            text = sub_feature.get("text", "")
                            if "sqft" in text and area_sqft == "æœªçŸ¥":
                                area_sqft = text
                    elif isinstance(feature_item, dict):
                        text = feature_item.get("text", "")
                        icon_name = feature_item.get("iconName", "")

                        if icon_name == "bed-o" and beds == "æœªçŸ¥":
                            beds = text
                        elif icon_name == "bath-o" and baths == "æœªçŸ¥":
                            baths = text
                        elif icon_name == "room-o" and beds == "æœªçŸ¥":
                            beds = text
                        elif "sqft" in text and area_sqft == "æœªçŸ¥":
                            area_sqft = text

            nearbyText = listingData.get("mrt", {}).get('nearbyText', 'æ— åœ°é“')
            badges = listingData.get("badges", [])

            built_year = "æœªçŸ¥"
            property_type = "æœªçŸ¥"
            tenure = "æœªçŸ¥"

            for badge in badges:
                badge_name = badge.get("name", "")
                badge_text = badge.get("text", "")

                if badge_name == "launch" and "Built:" in badge_text:
                    built_year = badge_text
                elif badge_name == "unit_type":
                    property_type = badge_text
                elif badge_name == "tenure":
                    tenure = badge_text

            if tenure == 'æœªçŸ¥':
                try:
                    tenure = listingData.get('additionalData', {}).get('tenure', 'æœªçŸ¥')
                except:
                    tenure = "æœªçŸ¥"

            recency_text = listingData.get("recency", {}).get("text", 'æ— æ›´æ–°æ—¶é—´')
            agent = listingData.get("agent", {})
            agent_id = agent.get("id", 'æ— id')
            agent_name = agent.get("name", 'æ— åå­—')
            agent_description = agent.get("description", 'æ— æè¿°')
            agent_url_path = agent.get("profileUrl")

            dic = {
                'ID': id_,
                "localizedTitle": localizedTitle,
                "fullAddress": fullAddress,
                "price_pretty": price_pretty,
                "beds": beds,
                "baths": baths,
                "area_sqft": area_sqft,
                "price_psf": price_psf,
                "nearbyText": nearbyText,
                "built_year": built_year,
                "property_type": property_type,
                "tenure": tenure,
                "url_path": url_path,
                "recency_text": recency_text,
                "agent_id": agent_id,
                "agent_name": agent_name,
                "agent_description": agent_description,
                "agent_url_path": agent_url_path,
                "CEA": '',
                "mobile": '',
                "rating": '',
                "buy_rent": html_name
            }
            self.insert_record(dic, force_update=force_update)

        return consecutive_exists, new_records

    def get_data(self, url_path, page, html_name, force_update=False):
        """è·å–é¡µé¢æ•°æ®"""
        if not force_update and self.check_spider_record(url_path):
            logger.info(f"é¡µé¢å·²çˆ¬å–: {url_path}")
            return 0, 0

        logger.info(f"å¼€å§‹è¯·æ±‚ï¼š{url_path}")
        response = self.fetch(url_path)
        if not response:
            logger.error(f"è¯·æ±‚å¤±è´¥ï¼š{url_path}")
            return 0, 0

        logger.info(f"è¯·æ±‚æˆåŠŸï¼š{url_path}")
        consecutive_exists, new_records = self.analysis_list_page(response, page, html_name, force_update)
        self.insert_spider_record(url_path, 'å·²çˆ¬å–')

        return consecutive_exists, new_records

    def crawl_category(self, category, start_page, end_page, incremental=True):
        """çˆ¬å–æŸä¸ªåˆ†ç±»ï¼ˆæ”¯æŒæ™ºèƒ½å¢é‡æ›´æ–°ï¼‰"""
        if incremental:
            last_page, _ = self.get_crawl_progress(category)

            # å¦‚æœæ˜¯æ–­ç‚¹ç»­çˆ¬ä¸”ä¸æ˜¯ä»ç¬¬1é¡µå¼€å§‹ï¼Œå…ˆå›æº¯æ£€æŸ¥å‰Né¡µ
            if last_page > 1:
                review_start = max(1, last_page - self.REVIEW_PAGES)
                logger.info(f"ğŸ”„ å›æº¯æ£€æŸ¥ç¬¬ {review_start}-{last_page - 1} é¡µï¼ˆå…±{last_page - review_start}é¡µï¼‰")

                for page in range(review_start, last_page):
                    url_path = f'{category}/{page}'
                    self.get_data(url_path, page, category, force_update=True)
                    time.sleep(1)

                start_page = last_page

        pages_without_new = 0  # è¿ç»­æ— æ–°è®°å½•çš„é¡µæ•°

        for page in range(start_page, end_page):
            url_path = f'{category}/{page}'
            consecutive_exists, new_records = self.get_data(url_path, page, category)

            # æ”¹è¿›çš„æ—©åœæ¡ä»¶ï¼šè¿ç»­Né¡µéƒ½æ²¡æœ‰æ–°è®°å½•
            if new_records == 0:
                pages_without_new += 1
                logger.info(f"âš ï¸  ç¬¬ {page} é¡µæ— æ–°è®°å½•ï¼ˆè¿ç»­ç¬¬{pages_without_new}é¡µï¼‰")
            else:
                pages_without_new = 0
                logger.info(f"âœ… ç¬¬ {page} é¡µæ–°å¢ {new_records} æ¡è®°å½•")

            # è¿ç»­å¤šé¡µéƒ½æ²¡æœ‰æ–°è®°å½•æ‰åœæ­¢
            if pages_without_new >= self.PAGES_WITHOUT_NEW_THRESHOLD:
                logger.warning(
                    f"è¿ç»­ {pages_without_new} é¡µæ— æ–°è®°å½•ï¼ˆé˜ˆå€¼: {self.PAGES_WITHOUT_NEW_THRESHOLD}ï¼‰ï¼Œåœæ­¢çˆ¬å–"
                )
                break

            self.update_crawl_progress(category, page + 1, end_page - 1)
            time.sleep(1)

        logger.success(f"{category} çˆ¬å–å®Œæˆ")

    def get_property_for_rent(self, start_page=1, end_page=1484, incremental=True):
        """çˆ¬å–ç§Ÿæˆ¿ä¿¡æ¯"""
        self.crawl_category('property-for-rent', start_page, end_page, incremental)

    def get_property_for_sale(self, start_page=1, end_page=2663, incremental=True):
        """çˆ¬å–ä¹°æˆ¿ä¿¡æ¯"""
        self.crawl_category('property-for-sale', start_page, end_page, incremental)

    def export_csv(self):
        """å¯¼å‡ºæ•°æ®åº“æ•°æ®åˆ°CSVæ–‡ä»¶"""
        try:
            export_dir = os.path.join(self.data_dir, "export")
            os.makedirs(export_dir, exist_ok=True)

            conn = sqlite3.connect(self.db_path)
            query = "SELECT * FROM propertyguru"
            df = pd.read_sql_query(query, conn)

            timestamp = time.strftime("%Y%m%d_%H%M%S")
            csv_path = os.path.join(export_dir, f"propertyguru_export_{timestamp}.csv")
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')

            rent_df = df[df['buy_rent'] == 'property-for-rent']
            sale_df = df[df['buy_rent'] == 'property-for-sale']

            rent_csv_path = os.path.join(export_dir, f"propertyguru_rent_{timestamp}.csv")
            sale_csv_path = os.path.join(export_dir, f"propertyguru_sale_{timestamp}.csv")

            rent_df.to_csv(rent_csv_path, index=False, encoding='utf-8-sig')
            sale_df.to_csv(sale_csv_path, index=False, encoding='utf-8-sig')

            stats = {
                "total_records": len(df),
                "rent_records": len(rent_df),
                "sale_records": len(sale_df),
                "export_time": timestamp
            }

            stats_path = os.path.join(export_dir, f"propertyguru_stats_{timestamp}.json")
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=4)

            logger.success(f"æ•°æ®å¯¼å‡ºæˆåŠŸ: {csv_path}")
            logger.success(f"ç§Ÿæˆ¿æ•°æ®: {rent_csv_path}, å…± {len(rent_df)} æ¡è®°å½•")
            logger.success(f"ä¹°æˆ¿æ•°æ®: {sale_csv_path}, å…± {len(sale_df)} æ¡è®°å½•")

            return csv_path

        except Exception as e:
            logger.error(f"å¯¼å‡ºCSVå¤±è´¥: {str(e)}")
            return None
        finally:
            if conn:
                conn.close()

    def main(self, mode='smart_incremental'):
        """
        ä¸»å‡½æ•°
        modeé€‰é¡¹ï¼š
        - 'full': å…¨é‡çˆ¬å–ï¼ˆä»ç¬¬1é¡µå¼€å§‹ï¼‰
        - 'incremental': ç®€å•å¢é‡ï¼ˆä»æ–­ç‚¹ç»§ç»­ï¼‰
        - 'smart_incremental': æ™ºèƒ½å¢é‡ï¼ˆè€ƒè™‘æ—¶æ•ˆæ€§ï¼Œè‡ªåŠ¨å†³å®šæ˜¯å¦å…¨é‡ï¼‰
        """
        logger.info(f"ğŸš€ å¼€å§‹çˆ¬å–ï¼Œæ¨¡å¼: {mode}")

        if mode == 'smart_incremental':
            # æ£€æŸ¥ä¸¤ä¸ªåˆ†ç±»çš„ä¸Šæ¬¡æ›´æ–°æ—¶é—´
            rent_last_update = self.get_last_crawl_time('property-for-rent')
            sale_last_update = self.get_last_crawl_time('property-for-sale')

            rent_days = (datetime.now() - rent_last_update).days
            sale_days = (datetime.now() - sale_last_update).days

            logger.info(f"ç§Ÿæˆ¿æ•°æ®ä¸Šæ¬¡æ›´æ–°: {rent_days} å¤©å‰")
            logger.info(f"ä¹°æˆ¿æ•°æ®ä¸Šæ¬¡æ›´æ–°: {sale_days} å¤©å‰")

            # å¦‚æœä»»ä¸€åˆ†ç±»è¶…è¿‡7å¤©æœªæ›´æ–°ï¼Œåˆ‡æ¢åˆ°å…¨é‡æ¨¡å¼
            if rent_days > 7 or sale_days > 7:
                logger.warning("è¶…è¿‡7å¤©æœªæ›´æ–°ï¼Œåˆ‡æ¢åˆ°å…¨é‡æ¨¡å¼")
                mode = 'full'

        if mode == 'full':
            logger.info("ğŸ“Š æ‰§è¡Œå…¨é‡çˆ¬å–")
            self.get_property_for_rent(start_page=1, incremental=False)
            self.get_property_for_sale(start_page=1, incremental=False)
        else:
            logger.info("âš¡ æ‰§è¡Œå¢é‡çˆ¬å–")
            self.get_property_for_rent(incremental=True)
            self.get_property_for_sale(incremental=True)

        # å¯¼å‡ºæ•°æ®åˆ°CSV
        self.export_csv()

        self.print_statistics()

        logger.success("ğŸ‰ æ‰€æœ‰ä»»åŠ¡å®Œæˆ")


    def print_statistics(self):
        """æ‰“å°å·®é‡æ›´æ–°ç»Ÿè®¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM propertyguru")
            total = cursor.fetchone()[0]
        except Exception as e:
            logger.error(f"è·å–æ€»è®°å½•æ•°å¤±è´¥: {str(e)}")
            total = "N/A"
        finally:
            if conn:
                conn.close()

        print(f"""
        ğŸ“Š æ›´æ–°ç»Ÿè®¡
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        æ€»è®°å½•æ•°: {total}
        æœ¬æ¬¡æ–°å¢: {self.new_count}
        æœ¬æ¬¡æ›´æ–°: {self.update_count}
        è·³è¿‡è®°å½•: {self.skip_count}
        å¤±è´¥è®°å½•: {self.fail_count}
        """)

    def print_statistics(self):
        """æ‰“å°å·®é‡æ›´æ–°ç»Ÿè®¡"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            cursor.execute("SELECT COUNT(*) FROM propertyguru")
            total = cursor.fetchone()[0]
        except Exception as e:
            logger.error(f"è·å–æ€»è®°å½•æ•°å¤±è´¥: {str(e)}")
            total = "N/A"
        finally:
            if conn:
                conn.close()

        print(f"""
        ğŸ“Š æ›´æ–°ç»Ÿè®¡
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        æ€»è®°å½•æ•°: {total}
        æœ¬æ¬¡æ–°å¢: {self.new_count}
        æœ¬æ¬¡æ›´æ–°: {self.update_count}
        è·³è¿‡è®°å½•: {self.skip_count}
        å¤±è´¥è®°å½•: {self.fail_count}
        """)


if __name__ == '__main__':
    pg = propertyguru()

    # æ–¹å¼1ï¼šæ™ºèƒ½æ¨¡å¼ï¼ˆæ¨èæ—¥å¸¸ä½¿ç”¨ï¼‰
    # è‡ªåŠ¨åˆ¤æ–­æ˜¯å¦éœ€è¦å…¨é‡ï¼Œè¶…è¿‡7å¤©è‡ªåŠ¨åˆ‡æ¢å…¨é‡
    pg.main(mode='smart_incremental')

    # æ–¹å¼2ï¼šæ¯å‘¨æ—¥å¼ºåˆ¶å…¨é‡
    # if datetime.now().weekday() == 6:
    #     pg.main(mode='full')

    # æ–¹å¼3ï¼šæ‰‹åŠ¨æŒ‡å®šæ¨¡å¼
    # pg.main(mode='full')  # å¼ºåˆ¶å…¨é‡
    # pg.main(mode='incremental')  # ç®€å•å¢é‡ï¼ˆä¸è€ƒè™‘æ—¶é—´çª—å£ï¼‰