import requests
import json
import time
import os
from loguru import logger
import re
from func_timeout import func_set_timeout
import urllib3
urllib3.disable_warnings()
import sqlite3
import pandas as pd
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock

logger.add("logs/propertyguru_pipeline.log", level="INFO")


class PropertyGuruPipeline:
    """PropertyGuru çˆ¬è™«å®Œæ•´æµç¨‹ - æ”¯æŒå¤šçº¿ç¨‹"""

    def __init__(self, max_workers=5):
        self.apikey = ''
        self.proxy = ''
        self.data_dir = "data"
        self.html_dir = os.path.join(self.data_dir, "html")
        self.json_dir = os.path.join(self.data_dir, "json")
        
        os.makedirs(self.html_dir, exist_ok=True)
        os.makedirs(self.json_dir, exist_ok=True)
        os.makedirs(self.data_dir, exist_ok=True)
        os.makedirs("logs", exist_ok=True)
        
        self.db_path = os.path.join(self.data_dir, "propertyguru_integrated.db")
        
        # Step 1 é…ç½®
        self.PAGES_WITHOUT_NEW_THRESHOLD = 5  # è¿ç»­æ— æ–°è®°å½•é¡µæ•°é˜ˆå€¼
        self.TIME_WINDOW_DAYS = 3  # æ—¶é—´çª—å£é˜ˆå€¼ï¼ˆå¤©æ•°ï¼‰
        self.REVIEW_PAGES = 10  # å›æº¯æ£€æŸ¥é¡µæ•°
        
        # Step 2 é…ç½®
        self.AGENT_INFO_EXPIRY_DAYS = 90  # ä»£ç†ä¿¡æ¯è¿‡æœŸæ—¶é—´ï¼ˆå¤©æ•°ï¼‰
        self.MAX_RETRIES = 3  # æœ€å¤§é‡è¯•æ¬¡æ•°
        
        # å¤šçº¿ç¨‹é…ç½®
        self.max_workers = max_workers
        self.db_lock = Lock()  # æ•°æ®åº“æ“ä½œé”
        
        self.init_database()

    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“ï¼Œåˆ›å»ºè¡¨ç»“æ„"""
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()

            # ä¸»æ•°æ®è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS propertyguru (
                    ID TEXT,
                    localizedTitle TEXT,
                    fullAddress TEXT,
                    price_pretty TEXT,
                    beds TEXT,
                    baths TEXT,
                    area_sqft TEXT,
                    price_psf TEXT,
                    nearbyText TEXT,
                    built_year TEXT,
                    property_type TEXT,
                    tenure TEXT,
                    url_path TEXT PRIMARY KEY,
                    recency_text TEXT,
                    agent_id TEXT,
                    agent_name TEXT,
                    agent_description TEXT,
                    agent_url_path TEXT,
                    CEA TEXT,
                    mobile TEXT,
                    rating TEXT,
                    buy_rent TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # çˆ¬è™«è®°å½•è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS propertyguru_spider (
                    url_path TEXT PRIMARY KEY,
                    status TEXT,
                    retry_count INTEGER DEFAULT 0,
                    last_error TEXT,
                    crawled_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # çˆ¬å–è¿›åº¦è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS crawl_progress (
                    category TEXT PRIMARY KEY,
                    last_page INTEGER,
                    total_pages INTEGER,
                    last_update TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            # å¤±è´¥è®°å½•è¡¨
            cursor.execute('''
                CREATE TABLE IF NOT EXISTS failed_records (
                    url_path TEXT PRIMARY KEY,
                    error_message TEXT,
                    retry_count INTEGER DEFAULT 0,
                    last_attempt TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            ''')

            conn.commit()
            logger.success(f"æ•°æ®åº“åˆå§‹åŒ–æˆåŠŸ: {self.db_path}")

        except Exception as e:
            logger.error(f"æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        finally:
            if conn:
                conn.close()

    # ==================== Step 1: åˆ—è¡¨é¡µçˆ¬å– ====================
    
    def get_crawl_progress(self, category):
        """è·å–çˆ¬å–è¿›åº¦ï¼Œè€ƒè™‘æ—¶é—´çª—å£"""
        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT last_page, last_update FROM crawl_progress WHERE category = ?",
                    (category,)
                )
                result = cursor.fetchone()

                if result:
                    last_page, last_update = result[0], result[1]
                    last_update_time = datetime.fromisoformat(last_update)
                    days_ago = (datetime.now() - last_update_time).days

                    if days_ago > self.TIME_WINDOW_DAYS:
                        logger.warning(
                            f"ä¸Šæ¬¡æ›´æ–°å·²è¿‡å» {days_ago} å¤©ï¼ˆé˜ˆå€¼: {self.TIME_WINDOW_DAYS}å¤©ï¼‰ï¼Œé‡æ–°å…¨é‡çˆ¬å–"
                        )
                        return 1, None

                    logger.info(f"ç»§ç»­ä¸Šæ¬¡è¿›åº¦ï¼Œä»ç¬¬ {last_page} é¡µå¼€å§‹")
                    return last_page, None

                logger.info(f"é¦–æ¬¡çˆ¬å– {category}")
                return 1, None

        except Exception as e:
            logger.error(f"è·å–çˆ¬å–è¿›åº¦å¤±è´¥: {str(e)}")
            return 1, None
        finally:
            if conn:
                conn.close()

    def update_crawl_progress(self, category, last_page, total_pages=None):
        """æ›´æ–°çˆ¬å–è¿›åº¦"""
        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                if total_pages:
                    cursor.execute(
                        "INSERT OR REPLACE INTO crawl_progress (category, last_page, total_pages, last_update) VALUES (?, ?, ?, ?)",
                        (category, last_page, total_pages, datetime.now())
                    )
                else:
                    cursor.execute(
                        "INSERT OR REPLACE INTO crawl_progress (category, last_page, last_update) VALUES (?, ?, ?)",
                        (category, last_page, datetime.now())
                    )
                conn.commit()
                logger.debug(f"æ›´æ–°çˆ¬å–è¿›åº¦: {category} ç¬¬ {last_page} é¡µ")
        except Exception as e:
            logger.error(f"æ›´æ–°çˆ¬å–è¿›åº¦å¤±è´¥: {str(e)}")
        finally:
            if conn:
                conn.close()

    def insert_spider_record(self, url_path, status, error_msg=None):
        """å‘çˆ¬è™«è®°å½•è¡¨ä¸­æ’å…¥è®°å½•"""
        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()

                cursor.execute("SELECT retry_count FROM propertyguru_spider WHERE url_path = ?", (url_path,))
                result = cursor.fetchone()
                retry_count = result[0] + 1 if result else 0

                cursor.execute('''
                    INSERT OR REPLACE INTO propertyguru_spider 
                    (url_path, status, retry_count, last_error, crawled_at) 
                    VALUES (?, ?, ?, ?, ?)
                ''', (url_path, status, retry_count, error_msg, datetime.now()))

                conn.commit()
        except Exception as e:
            logger.error(f"çˆ¬è™«è®°å½•æ’å…¥å¤±è´¥: {url_path}, é”™è¯¯: {str(e)}")
        finally:
            if conn:
                conn.close()

    def check_spider_record(self, url_path, force_update=False):
        """æ£€æŸ¥çˆ¬è™«è®°å½•è¡¨ä¸­æ˜¯å¦å­˜åœ¨æˆåŠŸè®°å½•"""
        if force_update:
            return False

        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                cursor.execute(
                    "SELECT status FROM propertyguru_spider WHERE url_path = ? AND status = 'å·²çˆ¬å–'",
                    (url_path,)
                )
                result = cursor.fetchone()
                return result is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥çˆ¬è™«è®°å½•å¤±è´¥: {url_path}, é”™è¯¯: {str(e)}")
            return False
        finally:
            if conn:
                conn.close()

    def insert_record(self, result, force_update=False, update_agent_only=False):
        """å‘æ•°æ®åº“ä¸­æ’å…¥æˆ–æ›´æ–°è®°å½•"""
        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()

                url_path = result.get("url_path", 'æ— url_path')
                cursor.execute("SELECT * FROM propertyguru WHERE url_path = ?", (url_path,))
                existing = cursor.fetchone()

                if existing:
                    if update_agent_only:
                        # åªæ›´æ–°ä»£ç†ä¿¡æ¯
                        cursor.execute('''
                            UPDATE propertyguru
                            SET CEA=?, mobile=?, rating=?, updated_at=?
                            WHERE url_path = ?
                        ''', (
                            result.get("CEA", ''),
                            result.get("mobile", ''),
                            result.get("rating", ''),
                            datetime.now(),
                            url_path
                        ))
                        logger.info(f"ä»£ç†ä¿¡æ¯æ›´æ–°æˆåŠŸ: {url_path}")
                    elif force_update:
                        # æ›´æ–°æ‰€æœ‰å­—æ®µ
                        cursor.execute('''
                            UPDATE propertyguru
                            SET ID=?, localizedTitle=?, fullAddress=?, price_pretty=?, beds=?, baths=?,
                                area_sqft=?, price_psf=?, nearbyText=?, built_year=?, property_type=?,
                                tenure=?, recency_text=?, agent_id=?, agent_name=?, agent_description=?,
                                agent_url_path=?, CEA=?, mobile=?, rating=?, buy_rent=?, updated_at=?
                            WHERE url_path = ?
                        ''', (
                            result.get("ID", 'æ— id'),
                            result.get("localizedTitle", 'æ— æ ‡é¢˜'),
                            result.get("fullAddress", 'æ— åœ°å€'),
                            result.get("price_pretty", 'æ— ä»·æ ¼'),
                            result.get("beds", 'æ— åºŠæ•°'),
                            result.get("baths", 'æ— æµ´å®¤æ•°'),
                            result.get("area_sqft", 'æ— é¢ç§¯'),
                            result.get("price_psf", 'æ— æ¯å¹³æ–¹è‹±å°ºä»·æ ¼'),
                            result.get("nearbyText", 'æ— åœ°é“'),
                            result.get("built_year", 'æ— å»ºé€ å¹´ä»½'),
                            result.get("property_type", 'æ— ç‰©ä¸šç±»å‹'),
                            result.get("tenure", 'æ— äº§æƒ'),
                            result.get("recency_text", 'æ— æ›´æ–°æ—¶é—´'),
                            result.get("agent_id", 'æ— id'),
                            result.get("agent_name", 'æ— åå­—'),
                            result.get("agent_description", 'æ— æè¿°'),
                            result.get("agent_url_path", 'æ— url_path'),
                            result.get("CEA", ''),
                            result.get("mobile", ''),
                            result.get("rating", ''),
                            result.get("buy_rent", 'æ— buy_rent'),
                            datetime.now(),
                            url_path
                        ))
                        logger.info(f"è®°å½•å¼ºåˆ¶æ›´æ–°: {url_path}")
                    else:
                        logger.debug(f"è®°å½•å·²å­˜åœ¨ï¼Œè·³è¿‡: {url_path}")
                        return False
                else:
                    # æ’å…¥æ–°è®°å½•
                    cursor.execute('''
                        INSERT INTO propertyguru (ID, localizedTitle, fullAddress, price_pretty, beds, baths,
                                                  area_sqft, price_psf, nearbyText, built_year, property_type,
                                                  tenure, url_path, recency_text, agent_id, agent_name,
                                                  agent_description, agent_url_path, CEA, mobile, rating, buy_rent)
                        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                    ''', (
                        result.get("ID", 'æ— id'),
                        result.get("localizedTitle", 'æ— æ ‡é¢˜'),
                        result.get("fullAddress", 'æ— åœ°å€'),
                        result.get("price_pretty", 'æ— ä»·æ ¼'),
                        result.get("beds", 'æ— åºŠæ•°'),
                        result.get("baths", 'æ— æµ´å®¤æ•°'),
                        result.get("area_sqft", 'æ— é¢ç§¯'),
                        result.get("price_psf", 'æ— æ¯å¹³æ–¹è‹±å°ºä»·æ ¼'),
                        result.get("nearbyText", 'æ— åœ°é“'),
                        result.get("built_year", 'æ— å»ºé€ å¹´ä»½'),
                        result.get("property_type", 'æ— ç‰©ä¸šç±»å‹'),
                        result.get("tenure", 'æ— äº§æƒ'),
                        url_path,
                        result.get("recency_text", 'æ— æ›´æ–°æ—¶é—´'),
                        result.get("agent_id", 'æ— id'),
                        result.get("agent_name", 'æ— åå­—'),
                        result.get("agent_description", 'æ— æè¿°'),
                        result.get("agent_url_path", 'æ— url_path'),
                        result.get("CEA", ''),
                        result.get("mobile", ''),
                        result.get("rating", ''),
                        result.get("buy_rent", 'æ— buy_rent')
                    ))
                    logger.info(f"è®°å½•æ’å…¥æˆåŠŸ: {url_path}")

                conn.commit()
                return True

        except Exception as e:
            logger.error(f"è®°å½•æ“ä½œå¤±è´¥: {url_path}, é”™è¯¯: {str(e)}")
            return False
        finally:
            if conn:
                conn.close()

    def check_record_exists(self, url_path):
        """æ£€æŸ¥è®°å½•æ˜¯å¦å­˜åœ¨"""
        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()
                cursor.execute("SELECT url_path FROM propertyguru WHERE url_path = ?", (url_path,))
                result = cursor.fetchone()
                return result is not None
        except Exception as e:
            logger.error(f"æ£€æŸ¥è®°å½•å¤±è´¥: {url_path}, é”™è¯¯: {str(e)}")
            return False
        finally:
            if conn:
                conn.close()

    @func_set_timeout(60)
    def get_request(self, method, url, headers):
        return requests.request(method, url, headers=headers, verify=False)

    def fetch(self, url_path, max_try=3):
        """è¯·æ±‚ç½‘é¡µ"""
        for attempt in range(max_try):
            try:
                url = f"https://api.cloudbypass.com/{url_path}"
                method = "GET"
                headers = {
                    "x-cb-apikey": f"{self.apikey}",
                    "x-cb-host": r"www.propertyguru.com.sg",
                    "x-cb-version": r"2",
                    "x-cb-part": r"0",
                    "x-cb-fp": r"chrome",
                    "x-cb-proxy": f"{self.proxy}",
                }

                response = self.get_request(method, url, headers)

                if response and response.status_code == 200:
                    return response
                else:
                    logger.error(f"è¯·æ±‚å¤±è´¥ç¬¬ {attempt + 1} æ¬¡: {url_path}")
                    if response:
                        code = response.json().get('code')
                        if code in ['CLOUDFLARE_CHALLENGE_TIMEOUT']:
                            continue
                        if code in ["PROXY_CONNECT_ABORTED", 'APIKEY_INVALID', 'INSUFFICIENT_BALANCE']:
                            logger.error(f"è‡´å‘½é”™è¯¯: {url_path} - {response.text}")
                            os._exit(0)
            except Exception as e:
                logger.error(f"è¯·æ±‚å¼‚å¸¸ç¬¬ {attempt + 1} æ¬¡: {url_path} - {str(e)}")
                continue
        return None

    def analysis_list_page(self, response, page, html_name, force_update=False):
        """è§£æåˆ—è¡¨é¡µ"""
        consecutive_exists = 0
        new_records = 0

        with open(os.path.join(self.html_dir, f'{html_name}_page_{page}.html'), 'w', encoding='utf-8') as f:
            f.write(response.text)

        data_json = re.findall('<script id="__NEXT_DATA__" type="application/json".*?>(.*?)</script>', 
                              response.text, re.S)
        if not data_json:
            logger.error(f"data_json è·å–å¤±è´¥ï¼š{page}")
            return consecutive_exists, new_records

        data_json = json.loads(data_json[0])

        with open(os.path.join(self.json_dir, f'{html_name}_page_{page}.json'), 'w', encoding='utf-8') as f:
            json.dump(data_json, f, ensure_ascii=False, indent=4)

        listingsData = data_json.get('props', {}).get('pageProps', {}).get('pageData', {}).get('data', {}).get(
            'listingsData', [])
        logger.info(f"{html_name} {page}é¡µæ•°æ®æ•°é‡ï¼š{len(listingsData)}")

        for item in listingsData:
            listingData = item.get('listingData', {})
            url_path = listingData.get("url", "").replace('https://www.propertyguru.com.sg/', '')

            if not force_update and self.check_record_exists(url_path):
                consecutive_exists += 1
                logger.debug(f"è®°å½•å·²å­˜åœ¨: {url_path} (è¿ç»­ç¬¬{consecutive_exists}æ¡)")
                continue
            else:
                consecutive_exists = 0
                new_records += 1

            # æå–æ•°æ®
            id_ = listingData.get('id', 'æ— id')
            localizedTitle = listingData.get('localizedTitle', 'æ— æ ‡é¢˜')
            fullAddress = listingData.get('fullAddress', 'æ— åœ°å€')
            price_pretty = listingData.get('price', {}).get('pretty', 'æ— ä»·æ ¼')

            beds = "æœªçŸ¥"
            baths = "æœªçŸ¥"
            area_sqft = "æœªçŸ¥"
            price_psf = "æœªçŸ¥"

            bedrooms = listingData.get('bedrooms')
            if bedrooms is not None and bedrooms >= 0:
                beds = f"{bedrooms} Beds"

            bathrooms = listingData.get('bathrooms')
            if bathrooms is not None and bathrooms >= 0:
                baths = f"{bathrooms} Baths"

            floorArea = listingData.get('floorArea')
            if floorArea:
                area_sqft = f"{floorArea} sqft"

            pricePerArea = listingData.get('pricePerArea', {}).get('localeStringValue')
            if pricePerArea:
                price_psf = f"S$ {pricePerArea} psf"

            listingFeatures = listingData.get('listingFeatures', [])
            if listingFeatures:
                for feature_item in listingFeatures:
                    if isinstance(feature_item, list):
                        for sub_feature in feature_item:
                            text = sub_feature.get("text", "")
                            if "sqft" in text and area_sqft == "æœªçŸ¥":
                                area_sqft = text
                    elif isinstance(feature_item, dict):
                        text = feature_item.get("text", "")
                        icon_name = feature_item.get("iconName", "")

                        if icon_name == "bed-o" and beds == "æœªçŸ¥":
                            beds = text
                        elif icon_name == "bath-o" and baths == "æœªçŸ¥":
                            baths = text
                        elif icon_name == "room-o" and beds == "æœªçŸ¥":
                            beds = text
                        elif "sqft" in text and area_sqft == "æœªçŸ¥":
                            area_sqft = text

            nearbyText = listingData.get("mrt", {}).get('nearbyText', 'æ— åœ°é“')
            badges = listingData.get("badges", [])

            built_year = "æœªçŸ¥"
            property_type = "æœªçŸ¥"
            tenure = "æœªçŸ¥"

            for badge in badges:
                badge_name = badge.get("name", "")
                badge_text = badge.get("text", "")

                if badge_name == "launch" and "Built:" in badge_text:
                    built_year = badge_text
                elif badge_name == "unit_type":
                    property_type = badge_text
                elif badge_name == "tenure":
                    tenure = badge_text

            if tenure == 'æœªçŸ¥':
                try:
                    tenure = listingData.get('additionalData', {}).get('tenure', 'æœªçŸ¥')
                except:
                    tenure = "æœªçŸ¥"

            recency_text = listingData.get("recency", {}).get("text", 'æ— æ›´æ–°æ—¶é—´')
            agent = listingData.get("agent", {})
            agent_id = agent.get("id", 'æ— id')
            agent_name = agent.get("name", 'æ— åå­—')
            agent_description = agent.get("description", 'æ— æè¿°')
            agent_url_path = agent.get("profileUrl")

            dic = {
                'ID': id_,
                "localizedTitle": localizedTitle,
                "fullAddress": fullAddress,
                "price_pretty": price_pretty,
                "beds": beds,
                "baths": baths,
                "area_sqft": area_sqft,
                "price_psf": price_psf,
                "nearbyText": nearbyText,
                "built_year": built_year,
                "property_type": property_type,
                "tenure": tenure,
                "url_path": url_path,
                "recency_text": recency_text,
                "agent_id": agent_id,
                "agent_name": agent_name,
                "agent_description": agent_description,
                "agent_url_path": agent_url_path,
                "CEA": '',
                "mobile": '',
                "rating": '',
                "buy_rent": html_name
            }
            self.insert_record(dic, force_update=force_update)

        return consecutive_exists, new_records

    def get_data(self, url_path, page, html_name, force_update=False):
        """è·å–é¡µé¢æ•°æ®"""
        if not force_update and self.check_spider_record(url_path):
            logger.info(f"é¡µé¢å·²çˆ¬å–: {url_path}")
            return 0, 0

        logger.info(f"å¼€å§‹è¯·æ±‚ï¼š{url_path}")
        response = self.fetch(url_path)
        if not response:
            logger.error(f"è¯·æ±‚å¤±è´¥ï¼š{url_path}")
            return 0, 0

        logger.info(f"è¯·æ±‚æˆåŠŸï¼š{url_path}")
        consecutive_exists, new_records = self.analysis_list_page(response, page, html_name, force_update)
        self.insert_spider_record(url_path, 'å·²çˆ¬å–')

        return consecutive_exists, new_records

    def crawl_category(self, category, start_page, end_page, incremental=True):
        """çˆ¬å–æŸä¸ªåˆ†ç±»ï¼ˆæ”¯æŒæ™ºèƒ½å¢é‡æ›´æ–°ï¼‰"""
        if incremental:
            last_page, _ = self.get_crawl_progress(category)

            if last_page > 1:
                review_start = max(1, last_page - self.REVIEW_PAGES)
                logger.info(f"ğŸ”„ å›æº¯æ£€æŸ¥ç¬¬ {review_start}-{last_page - 1} é¡µï¼ˆå…±{last_page - review_start}é¡µï¼‰")

                for page in range(review_start, last_page):
                    url_path = f'{category}/{page}'
                    self.get_data(url_path, page, category, force_update=True)
                    time.sleep(1)

                start_page = last_page

        pages_without_new = 0

        for page in range(start_page, end_page):
            url_path = f'{category}/{page}'
            consecutive_exists, new_records = self.get_data(url_path, page, category)

            if new_records == 0:
                pages_without_new += 1
                logger.info(f"âš ï¸  ç¬¬ {page} é¡µæ— æ–°è®°å½•ï¼ˆè¿ç»­ç¬¬{pages_without_new}é¡µï¼‰")
            else:
                pages_without_new = 0
                logger.info(f"âœ… ç¬¬ {page} é¡µæ–°å¢ {new_records} æ¡è®°å½•")

            if pages_without_new >= self.PAGES_WITHOUT_NEW_THRESHOLD:
                logger.warning(
                    f"è¿ç»­ {pages_without_new} é¡µæ— æ–°è®°å½•ï¼ˆé˜ˆå€¼: {self.PAGES_WITHOUT_NEW_THRESHOLD}ï¼‰ï¼Œåœæ­¢çˆ¬å–"
                )
                break

            self.update_crawl_progress(category, page + 1, end_page - 1)
            time.sleep(1)

        logger.success(f"{category} çˆ¬å–å®Œæˆ")

    def step1_crawl_listings(self, mode='smart_incremental'):
        """Step 1: çˆ¬å–æˆ¿äº§åˆ—è¡¨"""
        logger.info("=" * 60)
        logger.info("Step 1: å¼€å§‹çˆ¬å–æˆ¿äº§åˆ—è¡¨")
        logger.info("=" * 60)

        if mode == 'full':
            logger.info("ğŸ“Š æ‰§è¡Œå…¨é‡çˆ¬å–")
            self.crawl_category('property-for-rent', 1, 1484, incremental=False)
            self.crawl_category('property-for-sale', 1, 2663, incremental=False)
        else:
            logger.info("âš¡ æ‰§è¡Œå¢é‡çˆ¬å–")
            self.crawl_category('property-for-rent', 1, 1484, incremental=True)
            self.crawl_category('property-for-sale', 1, 2663, incremental=True)

        logger.success("Step 1 å®Œæˆï¼šæˆ¿äº§åˆ—è¡¨çˆ¬å–å®Œæˆ")

    # ==================== Step 2: è¯¦ç»†é¡µçˆ¬å–ï¼ˆå¤šçº¿ç¨‹ï¼‰ ====================

    def get_incomplete_records(self):
        """è·å–ä»£ç†ä¿¡æ¯ä¸å®Œæ•´çš„è®°å½•"""
        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()

                cursor.execute('''
                    SELECT url_path
                    FROM propertyguru
                    WHERE (CEA IS NULL OR CEA = '' OR CEA = 'æ— CEA')
                       OR (mobile IS NULL OR mobile = '' OR mobile = 'æ— æ‰‹æœº')
                       OR (rating IS NULL OR rating = '' OR rating = 'æ— è¯„åˆ†')
                ''')

                results = cursor.fetchall()
                url_paths = [row[0] for row in results]
                logger.info(f"æ‰¾åˆ° {len(url_paths)} æ¡ä»£ç†ä¿¡æ¯ä¸å®Œæ•´çš„è®°å½•")
                return url_paths

        except Exception as e:
            logger.error(f"è·å–ä¸å®Œæ•´è®°å½•å¤±è´¥: {str(e)}")
            return []
        finally:
            if conn:
                conn.close()

    def get_expired_records(self, days=None):
        """è·å–ä»£ç†ä¿¡æ¯è¿‡æœŸçš„è®°å½•"""
        if days is None:
            days = self.AGENT_INFO_EXPIRY_DAYS

        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()

                expiry_date = datetime.now() - timedelta(days=days)

                cursor.execute('''
                    SELECT url_path, updated_at
                    FROM propertyguru
                    WHERE updated_at < ?
                      AND CEA IS NOT NULL AND CEA != '' AND CEA != 'æ— CEA'
                      AND mobile IS NOT NULL AND mobile != '' AND mobile != 'æ— æ‰‹æœº'
                      AND rating IS NOT NULL AND rating != '' AND rating != 'æ— è¯„åˆ†'
                ''', (expiry_date,))

                results = cursor.fetchall()
                url_paths = [row[0] for row in results]

                if url_paths:
                    logger.info(f"æ‰¾åˆ° {len(url_paths)} æ¡ä»£ç†ä¿¡æ¯å·²è¿‡æœŸçš„è®°å½•ï¼ˆè¶…è¿‡{days}å¤©æœªæ›´æ–°ï¼‰")
                else:
                    logger.info(f"æ²¡æœ‰è¿‡æœŸçš„ä»£ç†ä¿¡æ¯ï¼ˆé˜ˆå€¼: {days}å¤©ï¼‰")

                return url_paths

        except Exception as e:
            logger.error(f"è·å–è¿‡æœŸè®°å½•å¤±è´¥: {str(e)}")
            return []
        finally:
            if conn:
                conn.close()

    def add_failed_record(self, url_path, error_msg):
        """æ·»åŠ å¤±è´¥è®°å½•"""
        try:
            with self.db_lock:
                conn = sqlite3.connect(self.db_path)
                cursor = conn.cursor()

                cursor.execute("SELECT retry_count FROM failed_records WHERE url_path = ?", (url_path,))
                result = cursor.fetchone()
                retry_count = result[0] + 1 if result else 1

                cursor.execute('''
                    INSERT OR REPLACE INTO failed_records 
                    (url_path, error_message, retry_count, last_attempt) 
                    VALUES (?, ?, ?, ?)
                ''', (url_path, error_msg, retry_count, datetime.now()))

                conn.commit()
                logger.warning(f"æ·»åŠ å¤±è´¥è®°å½•: {url_path}, é‡è¯•æ¬¡æ•°: {retry_count}")
        except Exception as e:
            logger.error(f"æ·»åŠ å¤±è´¥è®°å½•å¤±è´¥: {str(e)}")
        finally:
            if conn:
                conn.close()

    def get_property_detail(self, url_path):
        """è·å–è¯¦ç»†é¡µä»£ç†ä¿¡æ¯"""
        try:
            response = self.fetch(url_path, max_try=2)
            if not response:
                logger.error(f"è¯·æ±‚å¤±è´¥ï¼š{url_path}")
                return None

            file_name = url_path.replace('/', '_')
            with open(os.path.join(self.html_dir, f'detail_{file_name}.html'), 'w', encoding='utf-8') as f:
                f.write(response.text)

            data_json = re.findall('<script id="__NEXT_DATA__" type="application/json".*?>(.*?)</script>',
                                   response.text, re.S)
            if not data_json:
                logger.error(f"data_json è·å–å¤±è´¥ï¼š{url_path}")
                return None

            data_json = json.loads(data_json[0])

            with open(os.path.join(self.json_dir, f'detail_{file_name}.json'), 'w', encoding='utf-8') as f:
                json.dump(data_json, f, ensure_ascii=False, indent=4)

            agentInfoProps = data_json.get('props', {}).get('pageProps', {}).get('pageData', {}).get('data', {}).get(
                'contactAgentData', {}).get('contactAgentCard', {}).get("agentInfoProps", {})

            if not agentInfoProps:
                logger.warning(f"æœªæ‰¾åˆ°ä»£ç†ä¿¡æ¯: {url_path}")
                return {}

            agent = agentInfoProps.get('agent', {})
            description = re.sub(r'<[^>]*>', '', agent.get('description', 'æ— æè¿°'))
            mobile = agent.get('mobile', 'æ— æ‰‹æœº')

            rating = 'æ— è¯„åˆ†'
            rating_dic = agentInfoProps.get('rating', {})
            if rating_dic:
                rating = rating_dic.get('score', 'æ— è¯„åˆ†')

            dic = {
                "CEA": description,
                "mobile": mobile,
                "rating": rating
            }

            logger.info(f"æˆåŠŸè·å–ä»£ç†ä¿¡æ¯: {url_path}")
            return dic

        except Exception as e:
            logger.error(f"è·å–è¯¦ç»†é¡µå¤±è´¥: {url_path} - {str(e)}")
            return None

    def process_single_record(self, url_path, force_update=False):
        """å¤„ç†å•æ¡è®°å½•ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦å·²æˆåŠŸçˆ¬å–
        if not force_update and self.check_spider_record(url_path):
            return {'status': 'skipped', 'url_path': url_path}

        # è·å–ä»£ç†ä¿¡æ¯
        agent_detail = self.get_property_detail(url_path)

        if not agent_detail:
            self.add_failed_record(url_path, "è·å–ä»£ç†ä¿¡æ¯å¤±è´¥")
            self.insert_spider_record(url_path, 'å¤±è´¥', "è·å–ä»£ç†ä¿¡æ¯å¤±è´¥")
            return {'status': 'failed', 'url_path': url_path}

        # æ›´æ–°è®°å½•
        dic = {
            "url_path": url_path,
            "CEA": agent_detail.get("CEA", ''),
            "mobile": agent_detail.get("mobile", ''),
            "rating": agent_detail.get("rating", '')
        }

        if self.insert_record(dic, update_agent_only=True):
            self.insert_spider_record(url_path, 'å·²çˆ¬å–')
            return {'status': 'success', 'url_path': url_path}
        else:
            self.add_failed_record(url_path, "æ•°æ®åº“æ›´æ–°å¤±è´¥")
            return {'status': 'failed', 'url_path': url_path}

    def process_records_multithread(self, url_paths, force_update=False):
        """å¤šçº¿ç¨‹å¤„ç†è®°å½•"""
        if not url_paths:
            logger.info("æ²¡æœ‰éœ€è¦å¤„ç†çš„è®°å½•")
            return

        total = len(url_paths)
        success = 0
        failed = 0
        skipped = 0

        logger.info(f"å¼€å§‹å¤šçº¿ç¨‹å¤„ç† {total} æ¡è®°å½•ï¼Œçº¿ç¨‹æ•°: {self.max_workers}")

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_url = {
                executor.submit(self.process_single_record, url_path, force_update): url_path 
                for url_path in url_paths
            }

            # å¤„ç†å®Œæˆçš„ä»»åŠ¡
            for index, future in enumerate(as_completed(future_to_url), 1):
                url_path = future_to_url[future]
                try:
                    result = future.result()
                    
                    if result['status'] == 'success':
                        success += 1
                        logger.success(f"[{index}/{total}] âœ… æˆåŠŸ: {url_path}")
                    elif result['status'] == 'failed':
                        failed += 1
                        logger.error(f"[{index}/{total}] âŒ å¤±è´¥: {url_path}")
                    elif result['status'] == 'skipped':
                        skipped += 1
                        logger.info(f"[{index}/{total}] â­ï¸  è·³è¿‡: {url_path}")
                        
                    # æ˜¾ç¤ºè¿›åº¦
                    if index % 10 == 0:
                        logger.info(f"è¿›åº¦: {index}/{total} | æˆåŠŸ: {success} | å¤±è´¥: {failed} | è·³è¿‡: {skipped}")
                        
                except Exception as exc:
                    logger.error(f"[{index}/{total}] å¤„ç†å¼‚å¸¸: {url_path} - {str(exc)}")
                    failed += 1

                time.sleep(0.1)  # é¿å…è¯·æ±‚è¿‡å¿«

        logger.success(f"å¤šçº¿ç¨‹å¤„ç†å®Œæˆï¼æ€»æ•°: {total}, æˆåŠŸ: {success}, å¤±è´¥: {failed}, è·³è¿‡: {skipped}")

    def step2_crawl_agent_info(self, mode='incremental', expiry_days=None):
        """Step 2: çˆ¬å–ä»£ç†ä¿¡æ¯ï¼ˆå¤šçº¿ç¨‹ï¼‰"""
        logger.info("=" * 60)
        logger.info("Step 2: å¼€å§‹çˆ¬å–ä»£ç†ä¿¡æ¯ï¼ˆå¤šçº¿ç¨‹ï¼‰")
        logger.info("=" * 60)

        if mode == 'incremental':
            logger.info("âš¡ å·®é‡æ›´æ–°ï¼šè¡¥å……ç¼ºå¤±çš„ä»£ç†ä¿¡æ¯")
            url_paths = self.get_incomplete_records()
            self.process_records_multithread(url_paths, force_update=False)

        elif mode == 'expired':
            days = expiry_days if expiry_days else self.AGENT_INFO_EXPIRY_DAYS
            logger.info(f"â° è¿‡æœŸæ›´æ–°ï¼šæ›´æ–°è¶…è¿‡{days}å¤©çš„ä»£ç†ä¿¡æ¯")
            url_paths = self.get_expired_records(days)
            self.process_records_multithread(url_paths, force_update=True)

        else:
            logger.error(f"æœªçŸ¥çš„æ¨¡å¼: {mode}")
            return

        logger.success("Step 2 å®Œæˆï¼šä»£ç†ä¿¡æ¯çˆ¬å–å®Œæˆ")

    # ==================== å¯¼å‡ºåŠŸèƒ½ ====================

    def export_csv(self):
        """å¯¼å‡ºæ•°æ®åº“æ•°æ®åˆ°CSVæ–‡ä»¶"""
        try:
            export_dir = os.path.join(self.data_dir, "export")
            os.makedirs(export_dir, exist_ok=True)

            conn = sqlite3.connect(self.db_path)
            query = "SELECT * FROM propertyguru"
            df = pd.read_sql_query(query, conn)

            timestamp = time.strftime("%Y%m%d_%H%M%S")
            csv_path = os.path.join(export_dir, f"propertyguru_export_{timestamp}.csv")
            df.to_csv(csv_path, index=False, encoding='utf-8-sig')

            rent_df = df[df['buy_rent'] == 'property-for-rent']
            sale_df = df[df['buy_rent'] == 'property-for-sale']

            rent_csv_path = os.path.join(export_dir, f"propertyguru_rent_{timestamp}.csv")
            sale_csv_path = os.path.join(export_dir, f"propertyguru_sale_{timestamp}.csv")

            rent_df.to_csv(rent_csv_path, index=False, encoding='utf-8-sig')
            sale_df.to_csv(sale_csv_path, index=False, encoding='utf-8-sig')

            # ç»Ÿè®¡å®Œæ•´åº¦
            complete_records = len(df[
                (df['CEA'].notna()) & (df['CEA'] != '') & (df['CEA'] != 'æ— CEA') &
                (df['mobile'].notna()) & (df['mobile'] != '') & (df['mobile'] != 'æ— æ‰‹æœº') &
                (df['rating'].notna()) & (df['rating'] != '') & (df['rating'] != 'æ— è¯„åˆ†')
            ])

            stats = {
                "total_records": len(df),
                "rent_records": len(rent_df),
                "sale_records": len(sale_df),
                "complete_records": complete_records,
                "completion_rate": f"{complete_records / len(df) * 100:.2f}%" if len(df) > 0 else "0%",
                "export_time": timestamp
            }

            stats_path = os.path.join(export_dir, f"propertyguru_stats_{timestamp}.json")
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(stats, f, ensure_ascii=False, indent=4)

            logger.success(f"æ•°æ®å¯¼å‡ºæˆåŠŸ: {csv_path}")
            logger.success(f"ç§Ÿæˆ¿æ•°æ®: {rent_csv_path}, å…± {len(rent_df)} æ¡è®°å½•")
            logger.success(f"ä¹°æˆ¿æ•°æ®: {sale_csv_path}, å…± {len(sale_df)} æ¡è®°å½•")
            logger.success(f"å®Œæ•´è®°å½•: {complete_records}/{len(df)} ({stats['completion_rate']})")

            return csv_path

        except Exception as e:
            logger.error(f"å¯¼å‡ºCSVå¤±è´¥: {str(e)}")
            return None
        finally:
            if conn:
                conn.close()

    # ==================== ä¸»æµç¨‹ ====================

    def run_pipeline(self, step1_mode='smart_incremental', step2_mode='incremental', 
                    step2_expiry_days=None, skip_step1=False, skip_step2=False):
        """
        è¿è¡Œå®Œæ•´çš„Pipeline
        
        å‚æ•°:
        - step1_mode: Step 1æ¨¡å¼ ('full' æˆ– 'smart_incremental')
        - step2_mode: Step 2æ¨¡å¼ ('incremental' æˆ– 'expired')
        - step2_expiry_days: Step 2è¿‡æœŸå¤©æ•°ï¼ˆä»…å½“mode='expired'æ—¶ä½¿ç”¨ï¼‰
        - skip_step1: æ˜¯å¦è·³è¿‡Step 1
        - skip_step2: æ˜¯å¦è·³è¿‡Step 2
        """
        start_time = time.time()
        
        logger.info("ğŸš€" * 30)
        logger.info("PropertyGuru Pipeline å¯åŠ¨")
        logger.info("ğŸš€" * 30)

        try:
            # Step 1: çˆ¬å–åˆ—è¡¨é¡µ
            if not skip_step1:
                self.step1_crawl_listings(mode=step1_mode)
            else:
                logger.info("è·³è¿‡ Step 1")

            # Step 2: çˆ¬å–è¯¦ç»†é¡µï¼ˆå¤šçº¿ç¨‹ï¼‰
            if not skip_step2:
                self.step2_crawl_agent_info(mode=step2_mode, expiry_days=step2_expiry_days)
            else:
                logger.info("è·³è¿‡ Step 2")

            # å¯¼å‡ºæ•°æ®
            logger.info("=" * 60)
            logger.info("å¼€å§‹å¯¼å‡ºæ•°æ®")
            logger.info("=" * 60)
            self.export_csv()

            elapsed_time = time.time() - start_time
            logger.success(f"ğŸ‰ Pipeline å®Œæˆï¼æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")

        except Exception as e:
            logger.error(f"Pipeline æ‰§è¡Œå¤±è´¥: {str(e)}")
            raise


if __name__ == '__main__':
    # åˆ›å»ºPipelineå®ä¾‹ï¼ˆè®¾ç½®çº¿ç¨‹æ•°ï¼‰
    pipeline = PropertyGuruPipeline(max_workers=10)
    
    # ========== ä½¿ç”¨åœºæ™¯ç¤ºä¾‹ ==========
    
    # åœºæ™¯1: å®Œæ•´æµç¨‹ï¼ˆå¢é‡æ¨¡å¼ï¼‰- æ¨èæ—¥å¸¸ä½¿ç”¨
    pipeline.run_pipeline(
        step1_mode='smart_incremental',  # æ™ºèƒ½å¢é‡çˆ¬å–åˆ—è¡¨
        step2_mode='incremental',         # è¡¥å……ç¼ºå¤±çš„ä»£ç†ä¿¡æ¯
        skip_step1=False,
        skip_step2=False
    )
    
    # åœºæ™¯2: åªè¿è¡ŒStep 1ï¼ˆçˆ¬å–åˆ—è¡¨ï¼‰
    # pipeline.run_pipeline(
    #     step1_mode='smart_incremental',
    #     skip_step2=True
    # )
    
    # åœºæ™¯3: åªè¿è¡ŒStep 2ï¼ˆæ›´æ–°ä»£ç†ä¿¡æ¯ï¼‰
    # pipeline.run_pipeline(
    #     step2_mode='incremental',
    #     skip_step1=True
    # )
    
    # åœºæ™¯4: æ›´æ–°è¿‡æœŸçš„ä»£ç†ä¿¡æ¯ï¼ˆ90å¤©ï¼‰
    # pipeline.run_pipeline(
    #     step2_mode='expired',
    #     step2_expiry_days=90,
    #     skip_step1=True
    # )
    
    # åœºæ™¯5: å…¨é‡çˆ¬å–
    # pipeline.run_pipeline(
    #     step1_mode='full',
    #     step2_mode='incremental'
    # )
